<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Project - REACH Capstone Project</title>
  <link rel="stylesheet" href="./assets/css/styles.css">
</head>
<body>
  <!-- Navigation -->
<nav class="navbar navbar-expand-lg" style="background-color: #ffffff;">
    <div class="container d-flex align-items-center justify-content-between">
      
      <!-- Logo on the left -->
      <a class="navbar-brand" href="index.html">
        <img src="./assets/logo_navyText_whiteBG.svg" alt="REACH Logo" 
             style="height: 130px; max-height: 100%;" />
      </a>
  
      <!-- Collapsible Nav -->
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
              aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
  
      <!-- Links on the right -->
      <div class="collapse navbar-collapse justify-content-end" id="navbarNav">
        <ul class="navbar-nav text-end flex-lg-row flex-column">
          <li class="nav-item">
            <a class="nav-link active fw-semibold" 
               style="color:#001f4d; font-size: 1.25rem;" 
               href="index.html">Home</a>
          </li>
          <li class="nav-item">
            <a class="nav-link fw-semibold" 
               style="color:#001f4d; font-size: 1.25rem;" 
               href="team.html">Team</a>
          </li>
          <li class="nav-item">
            <a class="nav-link fw-semibold" 
               style="color:#001f4d; font-size: 1.25rem;" 
               href="project.html">Project</a>
          </li>
          <li class="nav-item">
            <a class="nav-link fw-semibold" 
               style="color:#001f4d; font-size: 1.25rem;" 
               href="documents.html">Documents</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>
  

  <!-- Main Content -->
  <div class="container mt-4">
    <div class="row">
      <div class="col-12">
        <h1 class="section-header">REACH Project</h1>
        <p class="lead">Complete project overview, solution, technologies, and timeline.</p>
      </div>
    </div>
    
    <!-- Project Overview -->
    <div class="row mb-4">
      <div class="col-12">
        <div class="card">
          <div class="card-body">
            <h3>Project Overview</h3>
            <p>
              The REACH project is developing a simulation-based reinforcement learning framework that trains a wearable
              robotic arm to perform everyday upper-limb tasks in a safe, repeatable virtual environment before deployment
              on hardware. By learning in simulation first, the system can explore many control strategies, reduce risk to
              participants and equipment, and ultimately provide smoother, more responsive, and energy-efficient assistance
              for stroke survivors and others with upper-limb impairments.
            </p>
            <p>
              <strong>Original Project Proposal:</strong> 
              <a href="https://www.ceias.nau.edu/cs/CS_Capstone/Projects/F25/20.RoboticArm_Lerner.pdf" target="_blank">View the initial concept provided by our sponsor</a>
            </p>
          </div>
        </div>
      </div>
    </div>

    <!-- Problem Statement -->
    <div class="row mb-4">
      <div class="col-12">
        <div class="card">
          <div class="card-body">
            <h3>The Problem</h3>
            <p class="lead">
              The overall problem that REACH aims to solve is the developmental pains of prototyping and testing assistive robotic devices.
            </p>
            <p>
              Current rehabilitation robotics research at NAU faces a significant bottleneck in control
system development. Modern assistive devices often rely on rigid, preprogrammed
control schemes that cannot easily adapt to dynamic, real-world human movement.
Because the learning and testing of new control algorithms occur primarily on physical
prototypes, each experimental iteration is slow, labor-intensive, and sometimes risky.
Without a robust simulation-based learning platform, progress in developing natural,
responsive, and safe robotic behaviors is limited by both hardware constraints and
human resource availability.
            </p>
            
          </div>
        </div>
      </div>
    </div>

    <!-- Requirements -->
    <div class="row mb-4">
      <div class="col-12">
        <div class="card">
          <div class="card-body">
            <h3>High-Level Requirements</h3>
            <p>For this project, our high-level requirements reflect the MUST-have functional requirements defined in our specification:</p>
            <ul>
              <li>A complete researcher interface that allows graduate students to run the full experiment workflow using Jupyter notebooks and YAML configuration files, without modifying core source code.</li>
              <li>A physics-accurate MuJoCo simulation environment with a waist-mounted robotic arm model, sensor simulation, basic reaching tasks, and gesture recognition and response capabilities configured through YAML.</li>
              <li>A reinforcement learning training loop that supports PPO and SAC with Stable-Baselines3, runs on both local machines and Monsoon HPC, logs metrics, saves and resumes checkpoints, and is fully driven by configuration files.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <!-- Solution Overview -->
    <div class="row mb-4">
      <div class="col-12">
        <div class="card">
          <div class="card-body">
            <h3>Technical Solution</h3>
            <p class="lead">
              REACH is a simulation-first reinforcement learning framework that accelerates the design and testing of upper-limb assistive robotic behaviors before they are deployed to real devices.
            </p>
            <p>
              The framework centers on a configurable virtual environment where an assistive robotic arm interacts with a simulated human partner using gesture recognition and response to practice cooperative upper-limb movements. Around this core, we provide flexible experiment configuration, scripted training workflows, and rich visual feedback so researchers can iterate on task goals, rewards, and motion strategies without modifying low-level control code. As the project matures, the same framework will be used to explore more complex rehabilitation and daily-living tasks, serving as a safe proving ground for assistive behaviors that will ultimately transfer to the physical hardware being developed with our Mechanical and Electrical Engineering collaborators.
            </p>                     
          </div>
        </div>
      </div>
    </div>

    <!-- System Architecture CURRENTLY HIDDEN -->
    <div class="row mb-4" style="display:none">
      <div class="col-12">
        <div class="card">
          <div class="card-body">
            <h3>System Architecture</h3>
            <p>
              TBD
            </p>
            <div class="placeholder">
              <h4>Architecture Diagram</h4>
              <p>TBD</p>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Implementation Details CURRENTLY HIDDEN -->
    <div class="row mb-4" style="display:none">
      <div class="col-12">
        <div class="card">
          <div class="card-body">
            <h3>Implementation Details</h3>
            <p>[Outline the implemented modules in high-level detail]</p>
            <div class="row">
              <div class="col-md-6">
                <h5>Core Modules</h5>
                <ul>
                  <li>TBD</li>
                  <li>TBD</li>
                  <li>TBD</li>
                </ul>
              </div>
              <div class="col-md-6">
                <h5>Key Features</h5>
                <ul>
                  <li>TBD</li>
                  <li>TBD</li>
                  <li>TBD</li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Technologies Used -->
    <div class="row mb-4">
      <div class="col-12">
        <div class="card">
          <div class="card-body">
            <h3>Technologies & Tools</h3>
            <div class="row">
              <div class="col-md-6 mb-3">
                <div class="card tech-card h-100">
                  <div class="card-body">
                    <h5 class="card-title">Python</h5>
                    <p class="card-text">
                      <strong>Why we chose it:</strong> Python offers a mature ecosystem for reinforcement learning, scientific computing, and research tooling, which lets us move quickly while keeping the code accessible to our collaborators.
                    </p>
                    <p class="card-text">
                      <strong>Role in project:</strong> Python is the primary language for our simulation environments, training workflow, configuration system, and analysis notebooks, tying the full REACH framework together for upper-limb assistive robotics.
                    </p>
                  </div>
                </div>
              </div>
              <div class="col-md-6 mb-3">
                <div class="card tech-card h-100">
                  <div class="card-body">
                    <h5 class="card-title">RL Stack (SB3 + PyTorch)</h5>
                    <p class="card-text">
                      <strong>Why we chose it:</strong> Stable-Baselines3 and PyTorch provide reliable, well-tested implementations of modern reinforcement learning algorithms that are widely used in research and easy to extend.
                    </p>
                    <p class="card-text">
                      <strong>Role in project:</strong> This stack powers the training loop for REACH, optimizing policies, managing checkpoints, and supporting algorithms like PPO and SAC for gesture and assistive-task learning.
                    </p>
                  </div>
                </div>
              </div>
            </div>
            <div class="row">
              <div class="col-md-6 mb-3">
                <div class="card tech-card h-100">
                  <div class="card-body">
                    <h5 class="card-title">MuJoCo Simulation</h5>
                    <p class="card-text">
                      <strong>Why we chose it:</strong> MuJoCo is a high-precision physics engine designed for robotics and control, making it well-suited for simulating upper-limb assistive devices and human interaction.
                    </p>
                    <p class="card-text">
                      <strong>Role in project:</strong> MuJoCo models the robotic arm, human, and environment so we can safely prototype tasks, test control strategies, and generate training data before running on real hardware.
                    </p>
                  </div>
                </div>
              </div>
              <div class="col-md-6 mb-3">
                <div class="card tech-card h-100">
                  <div class="card-body">
                    <h5 class="card-title">Gymnasium Environments</h5>
                    <p class="card-text">
                      <strong>Why we chose it:</strong> Gymnasium provides a standard interface for reinforcement learning environments, which keeps our tasks compatible with common RL libraries and tooling.
                    </p>
                    <p class="card-text">
                      <strong>Role in project:</strong> Our simulation tasks implement the Gymnasium API so that training scripts, evaluation tools, and future environments can plug into the REACH framework consistently.
                    </p>
                  </div>
                </div>
              </div>
            </div>
            <div class="row">
              <div class="col-md-6 mb-3">
                <div class="card tech-card h-100">
                  <div class="card-body">
                    <h5 class="card-title">Jupyter & YAML Config</h5>
                    <p class="card-text">
                      <strong>Why we chose it:</strong> Jupyter notebooks and human-readable YAML files make it easy for researchers to control experiments, document workflows, and share configurations without editing source code.
                    </p>
                    <p class="card-text">
                      <strong>Role in project:</strong> All major workflows—environment inspection, training, evaluation, and analysis—are driven through notebooks and configuration files, enabling reproducible experiments and rapid iteration.
                    </p>
                  </div>
                </div>
              </div>
              <div class="col-md-6 mb-3">
                <div class="card tech-card h-100">
                  <div class="card-body">
                    <h5 class="card-title">Vision & Gesture Recognition</h5>
                    <p class="card-text">
                      <strong>Why we chose it:</strong> Modern computer vision techniques are needed to detect human hands and objects accurately enough for meaningful gesture-based interaction.
                    </p>
                    <p class="card-text">
                      <strong>Role in project:</strong> Our vision pipeline processes simulated camera feeds to localize the user’s hand and other key features, feeding gesture information into the control policies that drive assistive motions.
                    </p>
                  </div>
                </div>
              </div>
            </div>
            <div class="row">
              <div class="col-md-6 mb-3">
                <div class="card tech-card h-100">
                  <div class="card-body">
                    <h5 class="card-title">Monsoon HPC & SLURM</h5>
                    <p class="card-text">
                      <strong>Why we chose it:</strong> Training rich reinforcement learning policies can be computationally expensive, so we leverage NAU’s Monsoon cluster and SLURM scheduler for scalable experiments.
                    </p>
                    <p class="card-text">
                      <strong>Role in project:</strong> Our training scripts can be launched locally or as SLURM jobs on Monsoon, allowing longer runs, parallel environments, and hyperparameter sweeps using the same configuration-driven workflow.
                    </p>
                  </div>
                </div>
              </div>
              <div class="col-md-6 mb-3">
                <div class="card tech-card h-100">
                  <div class="card-body">
                    <h5 class="card-title">GitHub & Collaboration</h5>
                    <p class="card-text">
                      <strong>Why we chose it:</strong> GitHub is a widely adopted platform for source control, code review, and team collaboration, which aligns well with our need to coordinate work across students, mentors, and sponsors.
                    </p>
                    <p class="card-text">
                      <strong>Role in project:</strong> GitHub hosts the REACH codebase, tracks issues and feature requests, manages pull requests and reviews, and serves as the central hub for documentation and release artifacts.
                    </p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Project Timeline -->
    <div class="row mb-4">
      <div class="col-12">
        <div class="card">
          <div class="card-body">
            <h3>Project Timeline</h3>
            <div class="row mb-3">
              <div class="col-md-4">
                <div class="text-center">
                  <h4 class="text-primary">Vision & Gesture Loop</h4>
                  <p class="text-muted">Current development phase (Dec 2025)</p>
                </div>
              </div>
              <div class="col-md-4">
                <div class="text-center">
                  <h4 class="text-success">May 2026</h4>
                  <p class="text-muted">Overall project completion target</p>
                </div>
              </div>
              <div class="col-md-4">
                <div class="text-center">
                  <h4 class="text-info">~5 months</h4>
                  <p class="text-muted">Time until project completion</p>
                </div>
              </div>
            </div>
            <div class="timeline-item">
              <h5>Phase 1: Simulation Foundations (Complete)</h5>
              <p class="text-muted">Aug – Sep 2025</p>
              <p>MuJoCo arm model, Gymnasium environment, reward shaping, and baseline reaching behaviors implemented and validated in simulation.</p>
            </div>
            <div class="timeline-item">
              <h5>Phase 2: Training Infrastructure (Complete)</h5>
              <p class="text-muted">Oct – Nov 2025</p>
              <p>PPO training wrapper, Monsoon SLURM pipeline, long-horizon training runs to 1M timesteps, and TensorBoard-based monitoring brought online.</p>
            </div>
            <div class="timeline-item">
              <h5>Phase 3: Vision & Gesture Loop (In Progress)</h5>
              <p class="text-muted">Dec 2025 – Feb 2026</p>
              <p>End-to-end loop for gesture recognition and response, including simulated camera feeds, gesture classification, and closed-loop policy training.</p>
            </div>
            <div class="timeline-item">
              <h5>Phase 4: Hardware Integration (Planned)</h5>
              <p class="text-muted">Mar – Apr 2026</p>
              <p>Integrate the learned policies with the physical robotic arm and perform simulation-to-real transfer testing in collaboration with the Biomechatronics Lab.</p>
            </div>
            <div class="timeline-item">
              <h5>Phase 5: Testing & Deployment (Planned)</h5>
              <p class="text-muted">Apr – May 2026</p>
              <p>Comprehensive testing, performance tuning, documentation, and final handoff of the REACH framework and artifacts to the Biomechatronics Lab.</p>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Screenshots/Demo -->
    <div class="row">
      <div class="col-12">
        <div class="card">
          <div class="card-body">
            <h3>Demo of Current Progress</h3>
            <p class="mb-3">Current MuJoCo reacher arm simulation demonstrating our baseline upper-limb assistive control behavior.</p>
            <video class="img-fluid rounded border" autoplay muted loop playsinline>
              <source src="./assets/reacher_arm_demo.mp4" type="video/mp4" />
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>

  <!-- Footer -->
  <footer class="bg-dark text-light mt-5 py-4">
    <div class="container text-center">
      <p class="mb-1">&copy; 2025 REACH Capstone Project. All rights reserved.</p>
      <p class="mb-0">
        See our code <a href="https://github.com/lucaslarson25/reach" target="_blank" 
        rel="noopener noreferrer" style="color: #ffffff; text-decoration: underline;">here</a>.
      </p>
    </div>
  </footer>

  <!-- Bootstrap JS (local) -->
  <script src="./assets/js/bootstrap.bundle.min.js"></script>
</body>
</html>