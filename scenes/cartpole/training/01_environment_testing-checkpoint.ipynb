{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Testing Notebook\n",
    "\n",
    "This notebook is for testing and debugging the REACH simulation environments.\n",
    "\n",
    "Use this to:\n",
    "- Test environment setup\n",
    "- Visualize observations and actions\n",
    "- Debug reward functions\n",
    "\n",
    "## FIRST TIME SETUP\n",
    "\n",
    "After you've cloned this repo, go to the root directory of the project (`cd PATH/TO/reach/`)\n",
    "\n",
    "Create a Python Virtual Environment (venv)\n",
    "\n",
    "`python -m venv .`\n",
    "\n",
    "VSCode might give you a popup about there being a new environment, asking if you want to set it to default. Select yes.\n",
    "<hr>\n",
    "Once you've created the environment, you don't need to do it again. However, you'll need to activate it each time you begin work (VSCode should do this automatically).\n",
    "\n",
    "If you need to do it manually:\n",
    "\n",
    "Unix: `source ./bin/activate`\n",
    "\n",
    "Windows PS: `./Scripts/Activate.ps1`\n",
    "\n",
    "This virtual environment keeps all the packages you install local to this project.\n",
    "<hr>\n",
    "\n",
    "In the top right of this notebook, you'll see a selector for the Python kernel (something like Python 3.12.1). Click on this and select \"Select another kernel\", then \"Python Environments\", then \"reach\".\n",
    "\n",
    "This ensures the Jupyter notebook runs your code inside the virtual environment.\n",
    "<hr>\n",
    "\n",
    "Next, you'll need to install the required packages.\n",
    "\n",
    "`pip install -r requirements.txt`\n",
    "<hr>\n",
    "\n",
    "Once you've completed the above steps, you should be able to test your environment. You can click \"Run All\" at the top or run the blocks one by one.\n",
    "\n",
    "If something doesn't work, try to do some research and see if you need to install prerequisite packages for your system. Otherwise, reach out to a REACH developer for assistance.\n",
    "\n",
    "## 1. Test Gymnasium and SB3\n",
    "\n",
    "Run the below code to test gymnasium and sb3 installations.\n",
    "\n",
    "There will be no window and it should only take about 3 seconds to run.\n",
    "\n",
    "NOTE: If you have a CUDA enabled GPU (NVIDIA), run `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121` to use GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.2     |\n",
      "|    ep_rew_mean     | 22.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 6092     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "✅ Completed cart pole test successfully!\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "def test_no_view():\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1, device=\"auto\")\n",
    "    model.learn(total_timesteps=500) # Increase this value to train longer, 500 takes about 1 minute, 5000 takes about 2.5 minutes\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(10):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "    env.close()\n",
    "    return model\n",
    "\n",
    "model = test_no_view()\n",
    "\n",
    "print(\"✅ Completed cart pole test successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test visualization\n",
    "\n",
    "Now, test the human view of the above code. This should create a pygame window (you might need to look for it!) and will take about 1 minute to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "pygame is not installed, run `pip install gymnasium[classic-control]`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS476/reach/reach/.venv/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:223\u001b[39m, in \u001b[36mCartPoleEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpygame\u001b[39;00m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpygame\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gfxdraw\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pygame'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDependencyNotInstalled\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m     env.close()\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m model = \u001b[43mtest_vis\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mtest_vis\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m env = gym.make(\u001b[33m\"\u001b[39m\u001b[33mCartPole-v1\u001b[39m\u001b[33m\"\u001b[39m, render_mode=\u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m model = PPO(\u001b[33m\"\u001b[39m\u001b[33mMlpPolicy\u001b[39m\u001b[33m\"\u001b[39m, env, verbose=\u001b[32m0\u001b[39m, device=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Increase this value to train longer, 100 takes about 40 seconds, 500 takes about 1 minute\u001b[39;00m\n\u001b[32m      6\u001b[39m obs, _ = env.reset()\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS476/reach/reach/.venv/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    307\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    308\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    313\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    314\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS476/reach/reach/.venv/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:287\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    277\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[32m    278\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    283\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    284\u001b[39m ) -> SelfOnPolicyAlgorithm:\n\u001b[32m    285\u001b[39m     iteration = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     total_timesteps, callback = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m     callback.on_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS476/reach/reach/.venv/lib/python3.11/site-packages/stable_baselines3/common/base_class.py:423\u001b[39m, in \u001b[36mBaseAlgorithm._setup_learn\u001b[39m\u001b[34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reset_num_timesteps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    422\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     \u001b[38;5;28mself\u001b[39m._last_obs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28mself\u001b[39m._last_episode_starts = np.ones((\u001b[38;5;28mself\u001b[39m.env.num_envs,), dtype=\u001b[38;5;28mbool\u001b[39m)\n\u001b[32m    425\u001b[39m     \u001b[38;5;66;03m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS476/reach/reach/.venv/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:77\u001b[39m, in \u001b[36mDummyVecEnv.reset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_envs):\n\u001b[32m     76\u001b[39m     maybe_options = {\u001b[33m\"\u001b[39m\u001b[33moptions\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._options[env_idx]} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._options[env_idx] \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     obs, \u001b[38;5;28mself\u001b[39m.reset_infos[env_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_seeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmaybe_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_obs(env_idx, obs)\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Seeds and options are only used once\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS476/reach/reach/.venv/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:83\u001b[39m, in \u001b[36mMonitor.reset\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m into reset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mself\u001b[39m.current_reset_info[key] = value\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS476/reach/reach/.venv/lib/python3.11/site-packages/gymnasium/wrappers/time_limit.py:75\u001b[39m, in \u001b[36mTimeLimit.reset\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[32m     67\u001b[39m \n\u001b[32m     68\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m \u001b[33;03m    The reset environment\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28mself\u001b[39m._elapsed_steps = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS476/reach/reach/.venv/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:61\u001b[39m, in \u001b[36mOrderEnforcing.reset\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m._has_reset = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS476/reach/reach/.venv/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:57\u001b[39m, in \u001b[36mPassiveEnvChecker.reset\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.checked_reset:\n\u001b[32m     56\u001b[39m     \u001b[38;5;28mself\u001b[39m.checked_reset = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_reset_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.reset(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS476/reach/reach/.venv/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:186\u001b[39m, in \u001b[36menv_reset_passive_checker\u001b[39m\u001b[34m(env, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m     logger.deprecation(\n\u001b[32m    182\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCurrent gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    183\u001b[39m     )\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# Checks the result of env.reset with kwargs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m result = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    189\u001b[39m     logger.warn(\n\u001b[32m    190\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    191\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS476/reach/reach/.venv/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:209\u001b[39m, in \u001b[36mCartPoleEnv.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28mself\u001b[39m.steps_beyond_terminated = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(\u001b[38;5;28mself\u001b[39m.state, dtype=np.float32), {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CS476/reach/reach/.venv/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:226\u001b[39m, in \u001b[36mCartPoleEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpygame\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gfxdraw\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[32m    227\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpygame is not installed, run `pip install gymnasium[classic-control]`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    228\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.screen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    231\u001b[39m     pygame.init()\n",
      "\u001b[31mDependencyNotInstalled\u001b[39m: pygame is not installed, run `pip install gymnasium[classic-control]`"
     ]
    }
   ],
   "source": [
    "def test_vis():\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=0, device=\"auto\")\n",
    "    model.learn(total_timesteps=100) # Increase this value to train longer, 100 takes about 40 seconds, 500 takes about 1 minute\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(10):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "    env.close()\n",
    "    return model\n",
    "\n",
    "model = test_vis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model\n",
    "\n",
    "Train the model for about 5-10 minutes and save its policy to ppo_cartpole.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "def train_model():\n",
    "    # 8 parallel environments\n",
    "    env = DummyVecEnv([lambda: gym.make(\"CartPole-v1\") for _ in range(8)])\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=0, device=\"auto\")  # verbose=0 to reduce logging\n",
    "\n",
    "    model.learn(total_timesteps=500_000) # This will take some time to complete\n",
    "    model.save(\"ppo_cartpole\")\n",
    "    env.close()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load model and record video\n",
    "\n",
    "This will load the policy we saved in the last step and it will run some tests while recording.\n",
    "\n",
    "The output video will be named `cartpole_trained.mp4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.video.io.ffmpeg_writer import FFMPEG_VideoWriter\n",
    "\n",
    "video_filename = \"cartpole_trained.mp4\"\n",
    "\n",
    "def load_and_record_model():\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    model = PPO.load(\"ppo_cartpole.zip\", env=env, device=\"cuda\")\n",
    "\n",
    "    fps = 60\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    first_frame = env.render()\n",
    "    height, width, _ = first_frame.shape\n",
    "\n",
    "    writer = FFMPEG_VideoWriter(\n",
    "        video_filename,\n",
    "        size=(width, height),\n",
    "        fps=fps,\n",
    "        codec=\"libx264\"\n",
    "    )\n",
    "\n",
    "    writer.write_frame(first_frame)\n",
    "\n",
    "    num_episodes = 10\n",
    "    max_steps_per_episode = 500\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        for step in range(max_steps_per_episode):\n",
    "            action, _ = model.predict(obs, deterministic=True)  # deterministic for video\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            frame = env.render()\n",
    "            if frame is not None:\n",
    "                writer.write_frame(frame)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "model = load_and_record_model()\n",
    "\n",
    "print(f\"✅ Video saved as {video_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reach-venv)",
   "language": "python",
   "name": "reach-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
